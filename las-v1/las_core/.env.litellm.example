# ==========================================
# LAS - LiteLLM Configuration
# ==========================================
# Copy this file to .env and fill in your API keys

# ==========================================
# DEFAULT PROVIDER SETTINGS
# ==========================================
# Default provider: ollama, openai, anthropic, gemini, groq, openrouter, huggingface, together_ai
LAS_DEFAULT_PROVIDER=ollama
LAS_DEFAULT_MODEL=llama3.2

# ==========================================
# LOCAL PROVIDERS
# ==========================================

# Ollama (Local - No API key required)
OLLAMA_BASE_URL=http://localhost:11434

# ==========================================
# CLOUD PROVIDER API KEYS
# ==========================================

# OpenAI
OPENAI_API_KEY=

# Anthropic (Claude)
ANTHROPIC_API_KEY=

# Google Gemini
GEMINI_API_KEY=
# Or use GOOGLE_API_KEY=

# Groq (Fast Inference)
GROQ_API_KEY=

# OpenRouter (Multi-provider gateway)
OPENROUTER_API_KEY=

# Together AI
TOGETHER_API_KEY=
# Or use TOGETHERAI_API_KEY=

# HuggingFace
HUGGINGFACE_API_KEY=
# Or use HF_TOKEN=

# DeepSeek
DEEPSEEK_API_KEY=

# Mistral
MISTRAL_API_KEY=

# Cohere
COHERE_API_KEY=

# Replicate
REPLICATE_API_KEY=

# ==========================================
# AZURE OPENAI
# ==========================================
AZURE_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=2024-02-15-preview

# ==========================================
# AWS BEDROCK
# ==========================================
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION_NAME=us-east-1

# ==========================================
# GOOGLE VERTEX AI
# ==========================================
VERTEX_PROJECT=
VERTEX_LOCATION=

# ==========================================
# CUSTOM OPENAI-COMPATIBLE ENDPOINTS
# ==========================================
# Use for self-hosted vLLM, LocalAI, LM Studio, etc.
CUSTOM_OPENAI_API_BASE=
CUSTOM_OPENAI_API_KEY=

# ==========================================
# LITELLM SETTINGS
# ==========================================
# Temperature (0.0 - 2.0)
LITELLM_TEMPERATURE=0.7

# Max tokens for generation
LITELLM_MAX_TOKENS=4096

# Enable caching
LITELLM_ENABLE_CACHING=true

# Enable cost tracking
LITELLM_ENABLE_COST_TRACKING=true

# Drop unsupported params instead of error
LITELLM_DROP_PARAMS=true

# ==========================================
# LOGGING & OBSERVABILITY
# ==========================================
# LangFuse (optional)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=

# Helicone (optional)
HELICONE_API_KEY=

# ==========================================
# RATE LIMITING
# ==========================================
# Requests per minute per model
LITELLM_RATE_LIMIT_PER_MODEL=60
